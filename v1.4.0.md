# Harvester v1.4.0 Release Notes

This release introduces several features, enhancements, and bug fixes that improve system quality and the overall user experience. The documentation is available at https://docs.harvesterhci.io/v1.4.

The Harvester team appreciates your contributions and looks forward to receiving feedback regarding this release.

## Downloads

### AMD64

#### Full ISO

Pending

#### Net Install ISO

Pending

### ARM64 (Technical Preview)

Pending

## Installation

Harvester can be installed using the [ISO image](https://docs.harvesterhci.io/v1.4/install/iso-install/), a [bootable USB drive](https://docs.harvesterhci.io/v1.4/install/usb-install), and [PXE boot](https://docs.harvesterhci.io/v1.4/install/pxe-boot-install/). A [net install ISO image](https://docs.harvesterhci.io/v1.4/install/net-install), which contains only the core OS components, is also now available. For more information, see the [Installation](https://docs.harvesterhci.io/v1.4/install/requirements) section of the documentation.

> [!IMPORTANT]
> The Harvester v1.4.0 installer checks if the hardware meets the [minimum requirements](https://docs.harvesterhci.io/v1.3/install/requirements/#hardware-requirements) for production use. If any of the checks fail, installation is stopped and warnings are printed to the system console.
> 
> You can disable this behavior during iPXE installation (for testing purposes) by adding the kernel parameter `harvester.install.skipchecks=true` when you boot the system. For more information, see [Useful Kernel Parameters](https://docs.harvesterhci.io/v1.4/install/pxe-boot-install#harvesterinstallskipcheckstrue).

## Upgrade

Harvester only allows upgrades from supported versions. For more information about upgrade paths and procedures, see [Upgrading Harvester](https://docs.harvesterhci.io/v1.4/upgrade/index).

## Highlights

### Technical Preview and Experimental Features

#### Longhorn V2 Data Engine Support

The [Longhorn V2 Data Engine](https://docs.harvesterhci.io/v1.4/advanced/longhorn-v2) harnesses the power of the Storage Performance Development Kit (SPDK) to significantly reduce I/O latency while boosting IOPS and throughput. The result is a high-performance storage solution that is capable of meeting diverse workload demands. 

You can enable the Longhorn V2 Data Engine either during installation using a configuration file, or after installation using the Harvester UI. If necessary, you can selectively disable the Longhorn V2 Data Engine by adding a label to target nodes (for example, nodes with less processing and memory resources).

In this release, you can only use the V2 Data Engine to create new volumes, which must be added to virtual machines as extra disks. More functionality, including live migration and volume cloning, will be added in future versions.

#### Volume Encryption and Decryption

Harvester v1.4.0 allows you to [encrypt and decrypt block volumes](https://docs.harvesterhci.io/v1.4/volume/volume-security), including boot volumes (virtual machine images) and data volumes, with a single key. This feature enhances security by ensuring that data stored in volumes is encrypted and can be decrypted when attached to virtual machines. 

The encryption mechanism utilizes the Linux kernel module dm_crypt and the command-line utility cryptsetup. Using the feature requires the creation of a Kubernetes secret to be used as the dm_crypt passphrase, and a StorageClass that contains encryption-related fields.

#### Local Storage Support

Pending

### Fully Supported Features

#### RWX Volumes for Guest Cluster Workloads

Workloads running on guest clusters may require read-write-many (RWX) volumes for backups and other purposes. Harvester v1.4 allows you to create RWX StorageClasses and associated volumes that can be attached to workloads through the Harvester CSI driver.

To create RWX volumes, you must first create an RWX StorageClass on both the host Harvester cluster and the guest cluster, and an associated RWX PersistentVolumeClaim (PVC). The RWX PVC must be specified when creating pods for workloads that require RWX volumes.

#### Scheduled Virtual Machine Backups

Harvester v1.4 supports the creation of virtual machine backups and snapshots on a [scheduled basis](https://docs.harvesterhci.io/v1.4/vm/backup-restore#scheduling-virtual-machine-backups-and-snapshots). You can suspend, resume, and update the schedule at runtime.

A cron expression is required for defining the schedule properties. You can specify the maximum number of up-to-date backups or snapshots to be retained. When this value is exceeded, Harvester deletes the oldest backups or snapshots.

#### Virtual Machine Snapshot Space Management

Volumes consume extra disk space in the cluster whenever you create a new virtual machine backup or snapshot. To manage this, you can configure [space usage limits](https://docs.harvesterhci.io/v1.4/vm/backup-restore#vm-snapshot-space-management) at the namespace and virtual machine levels. 

The configured values represent the maximum amount of disk space that can be used by all backups and snapshots. No limits are set by default.

#### Maintenance Mode Improvements

Harvester v1.4 allows you to define the behavior of individual virtual machines when [maintenance mode](https://docs.harvesterhci.io/v1.4/host/#node-maintenance) is enabled on a node. In earlier Harvester versions, enabling maintenance mode results in an attempt to migrate all virtual machines to other nodes in the cluster.

Using a new label, you can force certain virtual machines to shut down or restart instead of migrating to other nodes. The value of the label also determines if those virtual machines remain on the current node or are scheduled elsewhere. In addition, you can use a container lifecycle hook to execute a special command before shutting down a virtual machine.

#### USB Passthrough

[USB devices](https://docs.harvesterhci.io/v1.4/advanced/addons/pcidevices#usb-devices) that are attached to nodes can now be passed through by the Harvester hypervisor to allow direct access from virtual machines. This is accomplished through the pcidevices-controller add-on. 

USB passthrough is different from PCI passthrough in that individual devices (instead of the USB controller itself) are passed through. This allows the node and virtual machines to use USB devices in the same controller, but not the same device at the same time.

## Enhancements

Pending

## Bug Fixes

Pending

## Known Issues

Pending

## Components

| Component | Version |
| --- | --- |
| Longhorn | [v1.6.0](https://github.com/longhorn/longhorn/releases/tag/v1.6.0) |
| KubeVirt | [v1.1.0](https://github.com/kubevirt/kubevirt/releases/tag/v1.1.0) |
| Embedded Rancher | [v2.8.2](https://github.com/rancher/rancher/releases/tag/v2.8.2) |
| RKE2 | [v1.27.10+rke2r1](https://github.com/rancher/rke2/releases/tag/v1.27.10%2Brke2r1) |

# Contributors

Thank you to all the contributors that made this release possible.

Pending
